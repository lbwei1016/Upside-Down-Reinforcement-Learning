{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import torch.nn.functional as F\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init Environment\n",
    "# env = gym.make(\"CartPole-v0\")\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "action_space = env.action_space.n\n",
    "state_space = env.observation_space.shape[0]\n",
    "# max_reward = 200 # v0\n",
    "max_reward = 475  # v1\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "horizon_scale = 0.02\n",
    "return_scale = 0.02\n",
    "replay_size = 700\n",
    "n_warm_up_episodes = 50\n",
    "n_updates_per_iter = 100\n",
    "n_episodes_per_iter = 15\n",
    "last_few = 50\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Behavior Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BF(nn.Module):\n",
    "    def __init__(self, state_space, action_space, hidden_size, seed):\n",
    "        super(BF, self).__init__()\n",
    "        torch.manual_seed(seed)\n",
    "        self.actions = np.arange(action_space)\n",
    "        self.action_space = action_space\n",
    "        self.fc1 = nn.Linear(state_space, hidden_size)\n",
    "\n",
    "        # Commands consists of: (desired reward, desired horizon)\n",
    "        self.commands = nn.Linear(2, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc5 = nn.Linear(hidden_size, action_space)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, state, command):\n",
    "\n",
    "        out = self.sigmoid(self.fc1(state))\n",
    "        command_out = self.sigmoid(self.commands(command))\n",
    "        out = out * command_out\n",
    "        out = torch.relu(self.fc2(out))\n",
    "        out = torch.relu(self.fc3(out))\n",
    "        out = torch.relu(self.fc4(out))\n",
    "        out = self.fc5(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def action(self, state, desire, horizon):\n",
    "        \"\"\"\n",
    "        Samples the action based on their probability\n",
    "        \"\"\"\n",
    "        command = torch.cat((desire * return_scale, horizon * horizon_scale), dim=-1)\n",
    "        action_prob = self.forward(state, command)\n",
    "        probs = torch.softmax(action_prob, dim=-1)\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        return action\n",
    "\n",
    "    def greedy_action(self, state, desire, horizon):\n",
    "        \"\"\"\n",
    "        Returns the greedy action\n",
    "        \"\"\"\n",
    "        command = torch.cat((desire * return_scale, horizon * horizon_scale), dim=-1)\n",
    "        action_prob = self.forward(state, command)\n",
    "        probs = torch.softmax(action_prob, dim=-1)\n",
    "        action = torch.argmax(probs).item()\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size):\n",
    "        self.max_size = max_size\n",
    "        self.buffer = []\n",
    "\n",
    "    def add_sample(self, states, actions, rewards):\n",
    "        episode = {\n",
    "            \"states\": states,\n",
    "            \"actions\": actions,\n",
    "            \"rewards\": rewards,\n",
    "            \"summed_rewards\": sum(rewards),\n",
    "        }\n",
    "        self.buffer.append(episode)\n",
    "\n",
    "    def sort(self):\n",
    "        # sort buffer\n",
    "        self.buffer = sorted(\n",
    "            self.buffer, key=lambda i: i[\"summed_rewards\"], reverse=True\n",
    "        )\n",
    "        # keep the max buffer size\n",
    "        self.buffer = self.buffer[: self.max_size]\n",
    "\n",
    "    def get_random_samples(self, batch_size):\n",
    "        self.sort()\n",
    "        idxs = np.random.randint(0, len(self.buffer), batch_size)\n",
    "        batch = [self.buffer[idx] for idx in idxs]\n",
    "        return batch\n",
    "\n",
    "    def get_nbest(self, n):\n",
    "        self.sort()\n",
    "        return self.buffer[:n]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### init replay buffer with n-warmup runs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = ReplayBuffer(replay_size)\n",
    "bf = BF(state_space, action_space, 64, 1).to(device)\n",
    "optimizer = optim.Adam(params=bf.parameters(), lr=1e-3)\n",
    "\n",
    "samples = []\n",
    "# initial command\n",
    "init_desired_reward = 1\n",
    "init_time_horizon = 1\n",
    "\n",
    "for i in range(n_warm_up_episodes):\n",
    "    desired_return = torch.FloatTensor([init_desired_reward])\n",
    "    desired_time_horizon = torch.FloatTensor([init_time_horizon])\n",
    "    state = env.reset()[0]\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    while True:\n",
    "        action = bf.action(\n",
    "            torch.from_numpy(state).float().to(device),\n",
    "            desired_return.to(device),\n",
    "            desired_time_horizon.to(device),\n",
    "        )\n",
    "        # print(env.step(action.cpu().numpy()))\n",
    "        next_state, reward, done, _, _ = env.step(action.cpu().numpy())\n",
    "        # next_state, reward, done, _ = env.step(action.cpu().numpy())\n",
    "        states.append(torch.from_numpy(state).float())\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "\n",
    "        state = next_state\n",
    "        desired_return -= reward\n",
    "        desired_time_horizon -= 1\n",
    "        desired_time_horizon = torch.FloatTensor(\n",
    "            [np.maximum(desired_time_horizon, 1).item()]\n",
    "        )\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    buffer.add_sample(states, actions, rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# OBSERVE THE WEIGHTS before training\n",
    "# for p in bf.parameters():\n",
    "    # print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS FOR Sampling exploration commands\n",
    "def sampling_exploration(top_X_eps=last_few):\n",
    "    \"\"\"\n",
    "    This function calculates the new desired reward and new desired horizon based on the replay buffer.\n",
    "    New desired horizon is calculted by the mean length of the best last X episodes.\n",
    "    New desired reward is sampled from a uniform distribution given the mean and the std calculated from the last best X performances.\n",
    "    where X is the hyperparameter last_few.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    top_X = buffer.get_nbest(top_X_eps)\n",
    "    # The exploratory desired horizon dh0 is set to the mean of the lengths of the selected episodes\n",
    "    new_desired_horizon = np.mean([len(i[\"states\"]) for i in top_X])\n",
    "    # save all top_X cumulative returns in a list\n",
    "    returns = [i[\"summed_rewards\"] for i in top_X]\n",
    "    # from these returns calc the mean and std\n",
    "    mean_returns = np.mean(returns)\n",
    "    std_returns = np.std(returns)\n",
    "    # sample desired reward from a uniform distribution given the mean and the std\n",
    "    new_desired_reward = np.random.uniform(mean_returns, mean_returns + std_returns)\n",
    "\n",
    "    return torch.FloatTensor([new_desired_reward]), torch.FloatTensor(\n",
    "        [new_desired_horizon]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS FOR TRAINING\n",
    "def select_time_steps(saved_episode):\n",
    "    \"\"\"\n",
    "    Given a saved episode from the replay buffer this function samples random time steps (t1 and t2) in that episode:\n",
    "    T = max time horizon in that episode\n",
    "    Returns t1, t2 and T\n",
    "\n",
    "    t1 < t2 is guaranteed    \n",
    "    \"\"\"\n",
    "    # Select times in the episode:\n",
    "    T = len(saved_episode[\"states\"])  # episode max horizon\n",
    "    t1 = np.random.randint(0, T - 1)\n",
    "    t2 = np.random.randint(t1 + 1, T)\n",
    "\n",
    "    return t1, t2, T\n",
    "\n",
    "\n",
    "def create_training_input(episode, t1, t2):\n",
    "    \"\"\"\n",
    "    Based on the selected episode and the given time steps this function returns 4 values:\n",
    "    1. state at t1\n",
    "    2. the desired reward: sum over all rewards from t1 to t2\n",
    "    3. the time horizont: t2 - t1\n",
    "\n",
    "    4. the target action taken at t1\n",
    "\n",
    "    buffer episodes are build like [cumulative episode reward, states, actions, rewards]\n",
    "    \"\"\"\n",
    "    state = episode[\"states\"][t1]\n",
    "    desired_reward = sum(episode[\"rewards\"][t1:t2])\n",
    "    time_horizont = t2 - t1\n",
    "    action = episode[\"actions\"][t1]\n",
    "    return state, desired_reward, time_horizont, action\n",
    "\n",
    "\n",
    "def create_training_examples(batch_size):\n",
    "    \"\"\"\n",
    "    Creates a data set of training examples that can be used to create a data loader for training.\n",
    "    ============================================================\n",
    "    1. for the given batch_size episode idx are randomly selected\n",
    "    2. based on these episodes t1 and t2 are samples for each selected episode\n",
    "    3. for the selected episode and sampled t1 and t2 trainings values are gathered\n",
    "    ______________________________________________________________\n",
    "    Output are two numpy arrays in the length of batch size:\n",
    "    Input Array for the Behavior function - consisting of (state, desired_reward, time_horizon)\n",
    "    Output Array with the taken actions\n",
    "    \"\"\"\n",
    "    input_array = []\n",
    "    output_array = []\n",
    "    # select randomly episodes from the buffer\n",
    "    episodes = buffer.get_random_samples(batch_size)\n",
    "    for ep in episodes:\n",
    "        # select time stamps\n",
    "        t1, t2, T = select_time_steps(ep)\n",
    "        # For episodic tasks they set t2 to T:\n",
    "        t2 = T\n",
    "        state, desired_reward, time_horizont, action = create_training_input(ep, t1, t2)\n",
    "        input_array.append(\n",
    "            torch.cat(\n",
    "                [\n",
    "                    state,\n",
    "                    torch.FloatTensor([desired_reward]),\n",
    "                    torch.FloatTensor([time_horizont]),\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        output_array.append(action)\n",
    "    return input_array, output_array\n",
    "\n",
    "\n",
    "def train_behavior_function(batch_size):\n",
    "    \"\"\"\n",
    "    Trains the BF with on a cross entropy loss were the inputs are the action probabilities based on the state and command.\n",
    "    The targets are the actions appropriate to the states from the replay buffer.\n",
    "    \"\"\"\n",
    "    X, y = create_training_examples(batch_size)\n",
    "\n",
    "    X = torch.stack(X)\n",
    "    state = X[:, 0:state_space]\n",
    "    d = X[:, state_space : state_space + 1]\n",
    "    h = X[:, state_space + 1 : state_space + 2]\n",
    "    command = torch.cat((d * return_scale, h * horizon_scale), dim=-1)\n",
    "    y = torch.stack(y).long()\n",
    "    y_ = bf(state.to(device), command.to(device)).float()\n",
    "    optimizer.zero_grad()\n",
    "    pred_loss = F.cross_entropy(y_, y)\n",
    "    pred_loss.backward()\n",
    "    optimizer.step()\n",
    "    return pred_loss.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    desired_return=torch.FloatTensor([init_desired_reward]),\n",
    "    desired_time_horizon=torch.FloatTensor([init_time_horizon]),\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs one episode of the environment to evaluate the bf.\n",
    "    \"\"\"\n",
    "    state = env.reset()[0]\n",
    "    rewards = 0\n",
    "    while True:\n",
    "        state = torch.FloatTensor(state)\n",
    "        action = bf.action(\n",
    "            state.to(device), desired_return.to(device), desired_time_horizon.to(device)\n",
    "        )\n",
    "        # state, reward, done, _ = env.step(action.cpu().numpy())\n",
    "        state, reward, done, _, _ = env.step(action.cpu().numpy())\n",
    "        rewards += reward\n",
    "        desired_return = min(desired_return - reward, torch.FloatTensor([max_reward]))\n",
    "        desired_time_horizon = max(desired_time_horizon - 1, torch.FloatTensor([1]))\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm 2 - Generates an Episode unsing the Behavior Function:\n",
    "def generate_episode(\n",
    "    desired_return=torch.FloatTensor([init_desired_reward]),\n",
    "    desired_time_horizon=torch.FloatTensor([init_time_horizon]),\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates more samples for the replay buffer.\n",
    "    \"\"\"\n",
    "    state = env.reset()[0]\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    while True:\n",
    "        state = torch.FloatTensor(state)\n",
    "\n",
    "        action = bf.action(\n",
    "            state.to(device), desired_return.to(device), desired_time_horizon.to(device)\n",
    "        )\n",
    "        # next_state, reward, done, _ = env.step(action.cpu().numpy())\n",
    "        next_state, reward, done, _, _ = env.step(action.cpu().numpy())\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "\n",
    "        state = next_state\n",
    "        desired_return -= reward\n",
    "        desired_time_horizon -= 1\n",
    "        desired_time_horizon = torch.FloatTensor(\n",
    "            [np.maximum(desired_time_horizon, 1).item()]\n",
    "        )\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "    return [states, actions, rewards]\n",
    "\n",
    "\n",
    "# Algorithm 1 - Upside - Down Reinforcement Learning\n",
    "def run_upside_down(max_episodes):\n",
    "    \"\"\" \"\"\"\n",
    "    all_rewards = []\n",
    "    losses = []\n",
    "    # The moving average of the latest 100 rewards. \n",
    "    # If less than 100 episodes are finished, then just use what we've got.\n",
    "    average_100_reward = [] \n",
    "    desired_rewards_history = []\n",
    "    horizon_history = []\n",
    "    for ep in range(1, max_episodes + 1):\n",
    "\n",
    "        # improve|optimize bf based on replay buffer\n",
    "        loss_buffer = []\n",
    "        for i in range(n_updates_per_iter):\n",
    "            bf_loss = train_behavior_function(batch_size)\n",
    "            loss_buffer.append(bf_loss)\n",
    "        bf_loss = np.mean(loss_buffer)\n",
    "        losses.append(bf_loss)\n",
    "\n",
    "        # run x new episode and add to buffer\n",
    "        for i in range(n_episodes_per_iter):\n",
    "\n",
    "            # Sample exploratory commands based on buffer\n",
    "            new_desired_reward, new_desired_horizon = sampling_exploration()\n",
    "            generated_episode = generate_episode(\n",
    "                new_desired_reward, new_desired_horizon\n",
    "            )\n",
    "            buffer.add_sample(\n",
    "                generated_episode[0], generated_episode[1], generated_episode[2]\n",
    "            )\n",
    "\n",
    "        new_desired_reward, new_desired_horizon = sampling_exploration()\n",
    "        # monitoring desired reward and desired horizon\n",
    "        desired_rewards_history.append(new_desired_reward.item())\n",
    "        horizon_history.append(new_desired_horizon.item())\n",
    "\n",
    "        ep_rewards = evaluate(new_desired_reward, new_desired_horizon)\n",
    "        all_rewards.append(ep_rewards)\n",
    "        average_100_reward.append(np.mean(all_rewards[-100:]))\n",
    "\n",
    "        print(\n",
    "            \"\\rEpisode: {} | Rewards: {:.2f} | Mean_100_Rewards: {:.2f} | Loss: {:.2f}\".format(\n",
    "                ep, ep_rewards, np.mean(all_rewards[-100:]), bf_loss\n",
    "            ),\n",
    "            end=\"\",\n",
    "            flush=True,\n",
    "        )\n",
    "        if ep % 100 == 0:\n",
    "            print(\n",
    "                \"\\rEpisode: {} | Rewards: {:.2f} | Mean_100_Rewards: {:.2f} | Loss: {:.2f}\".format(\n",
    "                    ep, ep_rewards, np.mean(all_rewards[-100:]), bf_loss\n",
    "                )\n",
    "            )\n",
    "\n",
    "    return (\n",
    "        all_rewards,\n",
    "        average_100_reward,\n",
    "        desired_rewards_history,\n",
    "        horizon_history,\n",
    "        losses,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "import os\n",
    "\n",
    "name = \"model.pth\"\n",
    "if os.path.exists(name):\n",
    "    bf.load_state_dict(torch.load(name, weights_only=True))\n",
    "else:\n",
    "    rewards, average, d, h, loss = run_upside_down(max_episodes=2000)\n",
    "\n",
    "    plt.figure(figsize=(15,8))\n",
    "    plt.subplot(2,2,1)\n",
    "    plt.title(\"Rewards\")\n",
    "    plt.plot(rewards, label=\"rewards\")\n",
    "    plt.plot(average, label=\"average100\")\n",
    "    plt.legend()\n",
    "    plt.subplot(2,2,2)\n",
    "    plt.title(\"Loss\")\n",
    "    plt.plot(loss)\n",
    "    plt.subplot(2,2,3)\n",
    "    plt.title(\"desired Rewards\")\n",
    "    plt.plot(d)\n",
    "    plt.subplot(2,2,4)\n",
    "    plt.title(\"desired Horizon\")\n",
    "    plt.plot(h)\n",
    "    plt.show()\n",
    "\n",
    "    plt.savefig(\"imgs/UDRL/cartpole_2000.png\")\n",
    "\n",
    "    # SAVE MODEL\n",
    "    name = \"model.pth\"\n",
    "    torch.save(bf.state_dict(), name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OBSERVE THE WEIGHTS after training\n",
    "# for p in bf.parameters():\n",
    "#    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVALUATION RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DESIRED_REWARD = torch.FloatTensor([200]).to(device)\n",
    "DESIRED_HORIZON = torch.FloatTensor([200]).to(device)\n",
    "desired = DESIRED_REWARD.item()\n",
    "\n",
    "# `env` is defined earlier before training\n",
    "# env = gym.make(\"CartPole-v0\")\n",
    "env.reset()\n",
    "rewards = 0\n",
    "while True:\n",
    "    command = torch.cat(\n",
    "        (DESIRED_REWARD * return_scale, DESIRED_HORIZON * horizon_scale), dim=-1\n",
    "    )\n",
    "\n",
    "    probs_logits = bf(torch.from_numpy(state).float().to(device), command)\n",
    "    probs = torch.softmax(probs_logits, dim=-1).detach().cpu()\n",
    "    action = torch.argmax(probs).item()\n",
    "    # state, reward, done, info = env.step(action)\n",
    "    state, reward, done, info, _ = env.step(action)\n",
    "    rewards += reward\n",
    "    DESIRED_REWARD -= reward\n",
    "    DESIRED_HORIZON -= 1\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "print(\n",
    "    \"Desired rewards: {} | after finishing one episode the agent received {} rewards\".format(\n",
    "        desired, rewards\n",
    "    )\n",
    ")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
