{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ypw6MFWIovhC"
      },
      "source": [
        "# Upside-Down Reinforcement Learning\n",
        "\n",
        "This notebook provides an implementation of [UDRL](https://arxiv.org/abs/1912.02877) and reproduces the experiment in the paper on the LunarLander-v2 environment.\n",
        "\n",
        "To run an experiment (using CPU), simply execute all the sections below. The last section defines the experiment configuration, including a random seed. Each experiment is fully repeatable (if the machine and dependencies remain the same) by using the same seed.\n",
        "\n",
        "Tips:\n",
        "- The first agent evaluation during training will happen at 50 K, which should take about 100-120 seconds from start of training.\n",
        "- Set `verbose=True` in the config to see substantially more output.\n",
        "- The notebook includes the output for running the experiment for 3.7M environment steps after which this session timed out. The full experiment runs for 10M steps, as described in the paper.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mv4NQbMZSgeT"
      },
      "source": [
        "## Setup some basic dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 979
        },
        "id": "S2vicGMBmduX",
        "outputId": "fb364d23-41ac-4b51-ff3e-cfff7f19b665"
      },
      "outputs": [],
      "source": [
        "# !pip uninstall -y torch gym\n",
        "# !pip install torch==1.4.0+cpu gym[box2d]==0.15.4 tqdm sortedcontainers -f https://download.pytorch.org/whl/torch_stable.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOW8xhDXoiGH"
      },
      "source": [
        "## Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCCOEthAqwrP"
      },
      "source": [
        "### Replay Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "cellView": "code",
        "id": "Y2K3Ul0Op_jJ"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import os\n",
        "import pickle\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "import numpy as np\n",
        "from sortedcontainers import SortedListWithKey\n",
        "\n",
        "\n",
        "class Episode:\n",
        "    \"\"\"\n",
        "    For any episode, this container has len(actions) == len(rewards) == len(states) - 1\n",
        "    This is because we initialize using the starting state.\n",
        "    The add() method adds the action just taken, the obtained reward, and the **next** state.\n",
        "\n",
        "    This makes accessing the episode data simple:\n",
        "    states[0] is the first state\n",
        "    actions[0] is the action taken in that state\n",
        "    rewards[0] reward obtained by taking the action, and so on\n",
        "\n",
        "    The last state added is never actually processed by the agent.\n",
        "    \"\"\"\n",
        "    def __init__(self, init_state, desired_return, desired_horizon):\n",
        "        self.states = [init_state]\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "        self.desired_return = desired_return\n",
        "        self.desired_horizon = desired_horizon\n",
        "\n",
        "    def add(self, state, action, reward):\n",
        "        self.states.append(state)\n",
        "        self.actions.append(action)\n",
        "        self.rewards.append(reward)\n",
        "\n",
        "    @property\n",
        "    def total_reward(self):\n",
        "        return sum(self.rewards)\n",
        "\n",
        "    @property\n",
        "    def steps(self):\n",
        "        return len(self.actions)\n",
        "\n",
        "    @property\n",
        "    def return_gap(self):\n",
        "        return self.desired_return - self.total_reward\n",
        "\n",
        "    @property\n",
        "    def horizon_gap(self):\n",
        "        return self.desired_horizon - self.steps\n",
        "\n",
        "\n",
        "def get_reward(episode: Episode):\n",
        "    return episode.total_reward\n",
        "\n",
        "\n",
        "def make_replay(config):\n",
        "    if config.replay == 'highest':\n",
        "        replay = HighestReplay(max_size=config.replay_size)\n",
        "    elif config.replay == 'recent':\n",
        "        replay = RecentReplay(max_size=config.replay_size)\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "    return replay\n",
        "\n",
        "\n",
        "class Replay(ABC):\n",
        "    def __init__(self):\n",
        "        self.episodes = []\n",
        "        self.known_returns = []\n",
        "        self.known_horizons = []\n",
        "\n",
        "    @abstractmethod\n",
        "    def add(self, episode):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @property\n",
        "    def best_episode(self):\n",
        "        return max(self.episodes, key=get_reward)\n",
        "\n",
        "    def get_closest_horizon(self, desired_return):\n",
        "        idx = np.abs(np.asarray(self.known_returns) - desired_return).argmin()\n",
        "        return self.known_horizons[idx]\n",
        "\n",
        "    @property\n",
        "    def returns(self):\n",
        "        return [episode.total_reward for episode in self.episodes]\n",
        "\n",
        "\n",
        "class HighestReplay(Replay):\n",
        "    def __init__(self, max_size: int):\n",
        "        super().__init__()\n",
        "        self.episodes = SortedListWithKey(key=get_reward)\n",
        "        self.max_size = max_size\n",
        "\n",
        "    def add(self, episode: Episode):\n",
        "        self.episodes.add(episode)\n",
        "        self.known_returns.append(episode.total_reward)\n",
        "        self.known_horizons.append(episode.steps)\n",
        "        if len(self.episodes) > self.max_size:\n",
        "            self.episodes.pop(0)\n",
        "\n",
        "\n",
        "def trailing_segments(episode: Episode, nprnd: np.random.RandomState):\n",
        "    steps = episode.steps\n",
        "    i = nprnd.randint(0, steps)\n",
        "    j = steps\n",
        "    return episode.states[i], sum(episode.rewards[i:j]), (j - i), episode.actions[i]\n",
        "\n",
        "\n",
        "def sample_batch(replay: Replay, batch_size: int, nprnd: np.random.RandomState):\n",
        "    idxs = nprnd.randint(0, len(replay.episodes), batch_size)\n",
        "    episodes = [replay.episodes[idx] for idx in idxs]\n",
        "    segments = [trailing_segments(episode, nprnd) for episode in episodes]\n",
        "\n",
        "    states, desired_rewards, horizons, actions = [], [], [], []\n",
        "    for state, desired_reward, horizon, action in segments:\n",
        "        states.append(state)\n",
        "        desired_rewards.append(desired_reward)\n",
        "        horizons.append(horizon)\n",
        "        actions.append(action)\n",
        "\n",
        "    states = np.array(states, dtype=np.float32)\n",
        "    desired_rewards = np.array(desired_rewards, dtype=np.float32)[:, None]\n",
        "    horizons = np.array(horizons, dtype=np.float32)[:, None]\n",
        "    actions = np.array(actions, dtype=np.float32)\n",
        "\n",
        "    return states, desired_rewards, horizons, actions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nc1B-S01q-RI"
      },
      "source": [
        "### Behavior Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "zSQ2Nd4Ppprs"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from torch.nn.init import orthogonal_\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Categorical:\n",
        "    def __init__(self, dim: int):\n",
        "        self.dim = dim\n",
        "        self.loss = nn.CrossEntropyLoss(reduction='mean')\n",
        "\n",
        "    @staticmethod\n",
        "    def distribution(probs):\n",
        "        return torch.distributions.Categorical(probs)\n",
        "\n",
        "    def sample(self, scores):\n",
        "        probs = F.softmax(scores, dim=1)\n",
        "        dist = self.distribution(probs)\n",
        "        sample = dist.sample().item()                     # item() forces single env\n",
        "        return sample\n",
        "\n",
        "    def mode(self, scores):\n",
        "        probs = F.softmax(scores, dim=1)\n",
        "        mode = probs.to('cpu').data.numpy()[0].argmax()   # [0] forces single env\n",
        "        return mode\n",
        "\n",
        "    def random_sample(self):\n",
        "        return torch.randint(0, self.dim, (1,)).item()    # (1,) & item() force single env\n",
        "\n",
        "    def clip(self, action):\n",
        "        return action\n",
        "\n",
        "\n",
        "class ScaledIntent:\n",
        "    def __init__(self, return_scale: float, horizon_scale: float, max_return: float):\n",
        "        self.return_scale = return_scale\n",
        "        self.horizon_scale = horizon_scale\n",
        "        self.max_return = max_return\n",
        "\n",
        "    def __call__(self, intent):\n",
        "        _intent = np.zeros_like(intent)\n",
        "        returns = np.minimum(intent[:, 0], self.max_return)\n",
        "        horizons = np.maximum(intent[:, 1], 1)\n",
        "        _intent[:, 0] = returns * self.return_scale\n",
        "        _intent[:, 1] = horizons * self.horizon_scale\n",
        "        intent = _intent.astype(np.float32)\n",
        "        return intent\n",
        "\n",
        "\n",
        "def make_behavior_fn(config, nprnd: np.random.RandomState, device: torch.device):\n",
        "    intent_transform = ScaledIntent(config.return_scale, config.horizon_scale, config.env_max_return)\n",
        "    behavior_fn = BehaviorFn(ProductNetwork(config),\n",
        "                             intent_transform=intent_transform,\n",
        "                             config=config,\n",
        "                             nprnd=nprnd,\n",
        "                             device=device)\n",
        "    return behavior_fn\n",
        "\n",
        "\n",
        "class BehaviorFn(nn.Module):\n",
        "    def __init__(self, net: nn.Module, intent_transform: ScaledIntent, config, nprnd: np.random.RandomState, device=\"cpu\"):\n",
        "        super().__init__()\n",
        "        self.net = net\n",
        "        self.intent_transform = intent_transform\n",
        "        self.nprnd = nprnd\n",
        "        self.device = device\n",
        "        self.state_dtype = np.float32\n",
        "\n",
        "        if config.action_type == 'discrete':\n",
        "            self.action_dist = Categorical(config.n_action)\n",
        "            self.action_dtype = np.int64\n",
        "        else:\n",
        "            raise NotImplementedError(config.action_type)\n",
        "\n",
        "    def forward(self, state, desired_reward, horizon, device=None):\n",
        "        if device is None: device = self.device\n",
        "        state = np.asarray(state)\n",
        "        intent = np.concatenate([desired_reward, horizon], axis=1)\n",
        "        transformed_intent = self.intent_transform(intent)\n",
        "\n",
        "        state = self.make_variable(state, dtype=self.state_dtype, device=device)\n",
        "        intent = self.make_variable(transformed_intent, dtype=self.state_dtype, device=device)\n",
        "        net_output = self.net(state, intent)\n",
        "        if hasattr(self, 'logstd'):\n",
        "            net_output = torch.cat((net_output, self.logstd.expand_as(net_output)), dim=-1)\n",
        "        return net_output\n",
        "\n",
        "    def loss(self, states, desired_rewards, horizons, actions):\n",
        "        outputs = self(states, desired_rewards, horizons)\n",
        "        targets = self.make_variable(actions, dtype=self.action_dtype)\n",
        "        loss = self.action_dist.loss(outputs, targets)\n",
        "        return loss\n",
        "\n",
        "    def make_variable(self, x, dtype, device=None):\n",
        "        if device is None: device = self.device\n",
        "        return torch.from_numpy(np.asarray(x, dtype=dtype)).to(device)\n",
        "\n",
        "\n",
        "class ProductNetwork(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        activation, n_state, n_output, net_arch = config.activation, config.n_state, config.n_action, config.net_arch\n",
        "\n",
        "        self.n_state = n_state\n",
        "        self.net_option = config.net_option\n",
        "        n_proj = net_arch[0]\n",
        "\n",
        "        if activation == 'relu':\n",
        "            activation = nn.ReLU\n",
        "            gain = np.sqrt(2)\n",
        "        elif activation == 'tanh':\n",
        "            activation = nn.Tanh\n",
        "            gain = 1.0\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "        self.layer1 = FastWeightLayer(n_proj, n_state, 2, activation, option=self.net_option)\n",
        "        hids = []\n",
        "        n_last = n_proj\n",
        "        for n_current in net_arch[1:]:\n",
        "            hids += [nn.Linear(n_last, n_current), activation()]\n",
        "            n_last = n_current\n",
        "        self.hids = nn.Sequential(*hids)\n",
        "\n",
        "        self.op = nn.Linear(n_last, n_output)\n",
        "\n",
        "        self.init_params(gain)\n",
        "\n",
        "    def forward(self, state, intent):\n",
        "        out = self.layer1(state, intent)\n",
        "        out = self.hids(out) if len(self.hids) > 0 else out\n",
        "        out = self.op(out)\n",
        "        return out\n",
        "\n",
        "    def init_params(self, gain):\n",
        "        def init(m):\n",
        "            if type(m) == nn.Linear:\n",
        "                orthogonal_(m.weight.data, gain=gain)\n",
        "                m.bias.data.fill_(0.0)\n",
        "\n",
        "        def init_hyper(m):\n",
        "            if type(m) == nn.Linear:\n",
        "                orthogonal_(m.weight.data, gain=1.0)\n",
        "                m.bias.data.fill_(0.0)\n",
        "\n",
        "        self.layer1.apply(init_hyper)\n",
        "        self.hids.apply(init)\n",
        "\n",
        "\n",
        "class FastWeightLayer(nn.Module):\n",
        "    def __init__(self, size, x_size, c_size, activation, option):\n",
        "        super().__init__()\n",
        "        self.size = size\n",
        "        self.x_size = x_size\n",
        "        self.c_size = c_size\n",
        "        self.option = option\n",
        "        self.activation = activation\n",
        "\n",
        "        if option == 'bilinear':\n",
        "            self.Wlinear = nn.Linear(c_size, self.size * self.x_size)\n",
        "            self.blinear = nn.Linear(c_size, self.size)\n",
        "        elif option == 'gated':\n",
        "            self.xlinear = nn.Linear(x_size, size)\n",
        "            self.clinear = nn.Linear(c_size, size)\n",
        "        else:\n",
        "            raise NotImplementedError(option)\n",
        "\n",
        "    def forward(self, x, c):\n",
        "        if self.option == 'bilinear':\n",
        "            batch_size = x.shape[0]\n",
        "            W, b = self.Wlinear(c), self.blinear(c)\n",
        "            W = torch.reshape(W, (batch_size, self.x_size, self.size))\n",
        "            x = torch.reshape(x, (batch_size, 1, self.x_size))  # add a dimension for matmul, then remove it\n",
        "            output = self.activation()(torch.matmul(x, W).reshape((batch_size, self.size)) + b)\n",
        "        elif self.option == 'gated':\n",
        "            x_proj = self.activation()(self.xlinear(x))\n",
        "            c_proj = torch.sigmoid(self.clinear(c))\n",
        "            output = x_proj * c_proj\n",
        "        else:\n",
        "            raise NotImplementedError(self.option)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MynJcnRkrG2l"
      },
      "source": [
        "### Agent Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "uDbs9BoGnbrX"
      },
      "outputs": [],
      "source": [
        "from collections import deque\n",
        "from typing import List, Tuple, Union\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "from gym.core import Wrapper\n",
        "from tqdm.notebook import trange\n",
        "\n",
        "\n",
        "class SeedEnv(Wrapper):\n",
        "    \"\"\" Every reset() set a new seed from a given seed range \"\"\"\n",
        "    def __init__(self, env, seed_range):\n",
        "        super().__init__(env)\n",
        "        self.seed_range = seed_range\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        self.env.seed(np.random.randint(*self.seed_range))\n",
        "        return self.env.reset(**kwargs)\n",
        "\n",
        "    def step(self, action):\n",
        "        return self.env.step(action)\n",
        "\n",
        "\n",
        "def get_stats(scalar_list: list) -> dict:\n",
        "    if len(scalar_list) == 0:\n",
        "        stats = {key: np.nan for key in ('max', 'mean', 'median', 'min', 'std')}\n",
        "        stats['size'] = 0\n",
        "    else:\n",
        "        stats = {'max': np.max(scalar_list), 'mean': np.mean(scalar_list), 'median': np.median(scalar_list),\n",
        "                 'min': np.min(scalar_list), 'std': np.std(scalar_list, ddof=1), 'size': len(scalar_list)}\n",
        "    return stats\n",
        "\n",
        "\n",
        "class UpsideDownAgent:\n",
        "\n",
        "    def __init__(self, config):\n",
        "\n",
        "        self.config = config\n",
        "        self.msg = print if config.verbose else lambda *a, **k: None\n",
        "        self.device = torch.device('cuda:0' if config.use_gpu and torch.cuda.is_available() else 'cpu')\n",
        "        self.msg('Using device', self.device)\n",
        "\n",
        "        seed = config.seed\n",
        "        self.nprnd = np.random.RandomState(seed)\n",
        "        np.random.seed(Config.seed)\n",
        "        torch.manual_seed(seed)\n",
        "\n",
        "        self.train_env = SeedEnv(gym.make(config.env_name), seed_range=config.train_seeds)\n",
        "        self.test_env  = SeedEnv(gym.make(config.env_name), seed_range=config.eval_seeds)\n",
        "\n",
        "        self.replay: Replay = make_replay(config)\n",
        "        self.behavior_fn: BehaviorFn = make_behavior_fn(config, self.nprnd, self.device)\n",
        "        self.optimizer = torch.optim.Adam(self.behavior_fn.parameters(), lr=config.learning_rate)\n",
        "\n",
        "        self.iters = 0\n",
        "        self.total_episodes = 0\n",
        "        self.total_steps = 0\n",
        "        self.best_onpolicy_mean = np.array(-np.inf)\n",
        "        self.best_greedy_mean = np.array(-np.inf)\n",
        "        self.best_rolling_mean = np.array(-np.inf)\n",
        "        self.rolling_returns = deque(maxlen=config.n_eval_episodes)\n",
        "\n",
        "        self.current_step_limit = config.warmup_step_limit  # Used only for warm up inputs\n",
        "        self.current_desired_return = (config.warmup_desired_return, 0)\n",
        "\n",
        "    def warm_up(self) -> List[Tuple]:\n",
        "\n",
        "        results: List[Tuple] = []\n",
        "\n",
        "        episodes, _ = self.run_episodes(self.current_step_limit, self.current_desired_return, label='Warmup',\n",
        "                                        actions='random', n_episodes=self.config.n_warm_up_episodes)\n",
        "        self.total_episodes += self.config.n_warm_up_episodes\n",
        "        for episode in episodes:\n",
        "            self.replay.add(episode)\n",
        "            self.rolling_returns.append(episode.total_reward)\n",
        "\n",
        "        stats = get_stats(self.replay.returns)\n",
        "        self.msg(f\"\\nWarmup | Replay max: {stats['max']:7.2f} mean: {stats['mean']:7.2f} \"\n",
        "                 f\"min: {stats['min']:7.2f} size: {stats['size']:3}\")\n",
        "        results += [('replay.' + k, stats[k], self.total_steps) for k in ['max', 'mean', 'min', 'size']]\n",
        "\n",
        "        return results\n",
        "\n",
        "    def train_step(self) -> List[Tuple]:\n",
        "\n",
        "        results: List[Tuple] = []\n",
        "        n_updates = self.config.n_updates_per_iter\n",
        "        self.msg(f'\\nIteration {(self.iters + 1):3} | Training for {n_updates} updates')\n",
        "\n",
        "        # Learn behavior function\n",
        "        torch.set_grad_enabled(True)\n",
        "        self.behavior_fn.to(self.device)\n",
        "        self.behavior_fn.train()\n",
        "        loss = None\n",
        "        tq = trange(n_updates, disable=self.config.verbose is not True)\n",
        "        losses = []\n",
        "        for u in tq:\n",
        "            self.optimizer.zero_grad()\n",
        "            s, r, h, a = sample_batch(self.replay, self.config.batch_size, self.nprnd)\n",
        "            loss = self.behavior_fn.loss(s, r, h, a)\n",
        "            losses.append(loss.item())\n",
        "            tq.set_postfix(loss=loss.item())\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "        results += [('loss', loss.item(), self.total_steps)]\n",
        "\n",
        "        # Generate more data\n",
        "        last_few_episodes = self.replay.episodes[-self.config.last_few:]\n",
        "        last_few_durations = [episodes.steps for episodes in last_few_episodes]\n",
        "        last_few_returns = [episodes.total_reward for episodes in last_few_episodes]\n",
        "        self.current_step_limit = int(np.mean(last_few_durations))\n",
        "        self.current_desired_return = (np.mean(last_few_returns), np.std(last_few_returns))\n",
        "\n",
        "        episodes, eval_results = self.run_episodes(self.current_step_limit, self.current_desired_return,\n",
        "                                                   label='Train', actions=self.config.actions,\n",
        "                                                   n_episodes=self.config.n_episodes_per_iter)\n",
        "        self.total_episodes += self.config.n_episodes_per_iter\n",
        "\n",
        "        returns = []\n",
        "        for episode in episodes:\n",
        "            self.replay.add(episode)\n",
        "            episode_return = episode.total_reward\n",
        "            returns.append(episode_return)\n",
        "            self.rolling_returns.append(episode_return)\n",
        "        del episodes\n",
        "\n",
        "        # Logging\n",
        "        self.iters += 1\n",
        "        results += eval_results\n",
        "\n",
        "        rolling_mean = np.mean(self.rolling_returns)\n",
        "        results += [('rollouts.rolling_mean', rolling_mean, self.total_steps)]\n",
        "        self.best_rolling_mean = rolling_mean if rolling_mean > self.best_rolling_mean else self.best_rolling_mean\n",
        "\n",
        "        stats = get_stats(self.replay.returns)\n",
        "        self.msg(f\"Iteration {self.iters:3} | \"\n",
        "                 f\"Rollouts max: {np.max(returns):7.2f} mean: {np.mean(returns):7.2f} min: {np.min(returns):7.2f} | \"\n",
        "                 f\"Replay max: {stats['max']:7.2f} mean: {stats['mean']:7.2f} \"\n",
        "                 f\"min: {stats['min']: 7.2f} size: {stats['size']:3} | \"\n",
        "                 f\"steps so far: {self.total_steps:7} episodes so far: {self.total_episodes:6} | \"\n",
        "                 f\"Rolling Mean ({len(self.rolling_returns)}): {rolling_mean:7.2f}\")\n",
        "\n",
        "        results += [('iteration', self.iters, self.total_steps)]\n",
        "        results += [('current_step_limit', self.current_step_limit, self.total_steps)]\n",
        "        results += [('current_desired_return.mean', np.mean(last_few_returns), self.total_steps)]\n",
        "        results += [('current_desired_return.std', np.std(last_few_returns), self.total_steps)]\n",
        "        results += [('replay.' + k, stats[k], self.total_steps) for k in ['max', 'mean', 'min', 'size']]\n",
        "\n",
        "        stats = get_stats(returns)\n",
        "        results += [('rollouts.' + k, stats[k], self.total_steps) for k in ['max', 'mean', 'min']]\n",
        "        results += [('total_steps', self.total_steps, self.total_steps)]\n",
        "\n",
        "        return results\n",
        "\n",
        "    def _eval(self) -> List[Tuple]:\n",
        "\n",
        "        results: List[Tuple] = [('episodes', self.total_episodes, self.total_steps)]\n",
        "        if self.config.eval_goal == 'max':\n",
        "            desired_test_return = self.config.env_max_return\n",
        "        elif self.config.eval_goal == 'current':\n",
        "            desired_test_return = self.current_desired_return[0]\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "        actions = 'on_policy'  # can also evaluation with \"greedy\" actions here\n",
        "\n",
        "        self.msg(f'\\nTesting on {self.config.n_eval_episodes} episodes with {actions} actions')\n",
        "        episodes, _ = self.run_episodes(self.current_step_limit, desired_test_return, label='Test',\n",
        "                                        actions=actions, n_episodes=self.config.n_eval_episodes)\n",
        "        stats = get_stats([episode.total_reward for episode in episodes])\n",
        "        results += [(f'eval.{actions}.{k}', stats[k], self.total_steps)\n",
        "                    for k in ['max', 'median', 'mean', 'std', 'min']]\n",
        "        print(f\"Eval | {actions} | max: {stats['max']:7.2f} | median: {stats['median']:7.2f} | \"\n",
        "              f\"mean: {stats['mean']:7.2f} | std: {stats['std']: 7.2f} | min: {stats['min']:7.2f} | \"\n",
        "              f\"steps so far: {self.total_steps:7} | episodes so far: {self.total_episodes:6}\")\n",
        "\n",
        "        if actions == 'on_policy':\n",
        "            self.best_onpolicy_mean = max(stats['mean'], self.best_onpolicy_mean)\n",
        "        else:\n",
        "            self.best_greedy_mean = max(stats['mean'], self.best_greedy_mean)\n",
        "        del episodes, stats\n",
        "\n",
        "        return results\n",
        "\n",
        "\n",
        "    def run_episodes(self, step_limit: int, desired_return: Union[float, Tuple], label: str,\n",
        "                     actions: str, n_episodes: int = 1, render: bool = False) -> Tuple[List[Episode], List[Tuple]]:\n",
        "\n",
        "        assert label in ['Warmup', 'Train', 'Test']\n",
        "        assert actions in ['random', 'on_policy', 'greedy'] or actions.startswith('epsg')\n",
        "\n",
        "        behavior_fn, config, device, nprnd = self.behavior_fn, self.config, self.device, self.nprnd\n",
        "\n",
        "        torch.set_grad_enabled(False)\n",
        "        behavior_fn.eval()\n",
        "        behavior_fn.to(device)\n",
        "        episodes: List[Episode] = []\n",
        "        eval_results: List[Tuple] = []\n",
        "        env = self.test_env if label == 'Test' else self.train_env\n",
        "\n",
        "        for i in range(n_episodes):\n",
        "            if isinstance(desired_return, tuple):\n",
        "                desired_return_final = desired_return[0] + desired_return[1] * nprnd.random_sample()\n",
        "            else:\n",
        "                desired_return_final = desired_return\n",
        "            if config.env_name == 'TakeCover-v0' or config.env_name == 'CartPoleContinuous-v0':\n",
        "                desired_return_final = np.int(desired_return_final)\n",
        "                step_limit = desired_return_final\n",
        "\n",
        "            # Prepare env\n",
        "            state = env.reset()\n",
        "            if render: env.render()\n",
        "\n",
        "            # Generate episode\n",
        "            episode = Episode(state, desired_return_final, step_limit)\n",
        "            done = False\n",
        "\n",
        "            while episode.steps < config.env_step_limit and not done:\n",
        "                state = np.asarray(state)\n",
        "\n",
        "                if actions == 'random':\n",
        "                    action = behavior_fn.action_dist.random_sample()\n",
        "                elif actions == 'on_policy':\n",
        "                    desired_return_remaining = np.array([[desired_return_final - episode.total_reward]])\n",
        "                    steps_remaining = np.array([[step_limit - episode.steps]])\n",
        "                    action_scores = behavior_fn(state[None],\n",
        "                                                desired_return_remaining,\n",
        "                                                steps_remaining,\n",
        "                                                device=device)\n",
        "                    action = behavior_fn.action_dist.sample(action_scores)\n",
        "                else:\n",
        "                    raise NotImplementedError\n",
        "\n",
        "                clipped_action = behavior_fn.action_dist.clip(action)\n",
        "                state, reward, done, _ = env.step(clipped_action)\n",
        "                if render: env.render()\n",
        "\n",
        "                if label == 'Test':\n",
        "                    episode.add(0, 0, reward)  # reduce memory usage\n",
        "                else:\n",
        "                    episode.add(state, clipped_action, reward)\n",
        "                    self.total_steps += 1\n",
        "                    if label == 'Train' and self.total_steps % self.config.eval_freq == 0:\n",
        "                        eval_results += self._eval()\n",
        "\n",
        "            self.msg(f'{label} | {actions} | Episode {i:3} | '\n",
        "                     f'Goals: ({desired_return_final:7.2f}, {step_limit:4}) | '\n",
        "                     f'Return: {episode.total_reward:7.2f} Steps: {episode.steps:4} | '\n",
        "                     f'Return gap: {episode.return_gap:7.2f} Horizon gap: {episode.horizon_gap:5} ')\n",
        "\n",
        "            episodes.append(episode)\n",
        "\n",
        "        return episodes, eval_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxZ2fdF9rMww"
      },
      "source": [
        "# Run Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "If4nafnjnFOf",
        "outputId": "e9f577fd-c315-48e6-9e85-2e3324efc941"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warm-up complete. Starting training.\n",
            "Eval | on_policy | max:   15.14 | median:  -81.41 | mean:  -92.21 | std:   48.86 | min: -271.68 | steps so far:   50000 | episodes so far:    570\n",
            "Eval | on_policy | max:   19.47 | median: -127.37 | mean: -116.37 | std:   57.48 | min: -300.72 | steps so far:  100000 | episodes so far:   1170\n",
            "Eval | on_policy | max:   54.11 | median: -125.85 | mean: -111.51 | std:   73.26 | min: -323.89 | steps so far:  150000 | episodes so far:   1730\n",
            "Eval | on_policy | max:   73.36 | median: -136.19 | mean: -132.29 | std:   74.57 | min: -503.45 | steps so far:  200000 | episodes so far:   2170\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[20], line 57\u001b[0m\n\u001b[1;32m     54\u001b[0m eval_means, eval_medians \u001b[38;5;241m=\u001b[39m [], []\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m ud\u001b[38;5;241m.\u001b[39mtotal_steps \u001b[38;5;241m<\u001b[39m Config\u001b[38;5;241m.\u001b[39mmax_training_steps:\n\u001b[0;32m---> 57\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mud\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[1;32m     59\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m r[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval.on_policy.mean\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
            "Cell \u001b[0;32mIn[19], line 116\u001b[0m, in \u001b[0;36mUpsideDownAgent.train_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_step_limit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39mmean(last_few_durations))\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_desired_return \u001b[38;5;241m=\u001b[39m (np\u001b[38;5;241m.\u001b[39mmean(last_few_returns), np\u001b[38;5;241m.\u001b[39mstd(last_few_returns))\n\u001b[0;32m--> 116\u001b[0m episodes, eval_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_episodes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_step_limit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_desired_return\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mn_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_episodes_per_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_episodes \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mn_episodes_per_iter\n\u001b[1;32m    121\u001b[0m returns \u001b[38;5;241m=\u001b[39m []\n",
            "Cell \u001b[0;32mIn[19], line 237\u001b[0m, in \u001b[0;36mUpsideDownAgent.run_episodes\u001b[0;34m(self, step_limit, desired_return, label, actions, n_episodes, render)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n\u001b[1;32m    236\u001b[0m clipped_action \u001b[38;5;241m=\u001b[39m behavior_fn\u001b[38;5;241m.\u001b[39maction_dist\u001b[38;5;241m.\u001b[39mclip(action)\n\u001b[0;32m--> 237\u001b[0m state, reward, done, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclipped_action\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m render: env\u001b[38;5;241m.\u001b[39mrender()\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m label \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
            "Cell \u001b[0;32mIn[19], line 22\u001b[0m, in \u001b[0;36mSeedEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/ML/rl/lib/python3.10/site-packages/gym/wrappers/time_limit.py:17\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m---> 17\u001b[0m     observation, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
            "File \u001b[0;32m~/ML/rl/lib/python3.10/site-packages/gym/wrappers/order_enforcing.py:13\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 13\u001b[0m     observation, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m observation, reward, done, info\n",
            "File \u001b[0;32m~/ML/rl/lib/python3.10/site-packages/gym/envs/box2d/lunar_lander.py:336\u001b[0m, in \u001b[0;36mLunarLander.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    334\u001b[0m tip \u001b[38;5;241m=\u001b[39m (math\u001b[38;5;241m.\u001b[39msin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlander\u001b[38;5;241m.\u001b[39mangle), math\u001b[38;5;241m.\u001b[39mcos(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlander\u001b[38;5;241m.\u001b[39mangle))\n\u001b[1;32m    335\u001b[0m side \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m-\u001b[39mtip[\u001b[38;5;241m1\u001b[39m], tip[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m--> 336\u001b[0m dispersion \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnp_random\u001b[38;5;241m.\u001b[39muniform(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1.0\u001b[39m) \u001b[38;5;241m/\u001b[39m SCALE \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m)]\n\u001b[1;32m    338\u001b[0m m_power \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontinuous \u001b[38;5;129;01mand\u001b[39;00m action[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontinuous \u001b[38;5;129;01mand\u001b[39;00m action \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    341\u001b[0m ):\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;66;03m# Main engine\u001b[39;00m\n",
            "File \u001b[0;32m~/ML/rl/lib/python3.10/site-packages/gym/envs/box2d/lunar_lander.py:336\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    334\u001b[0m tip \u001b[38;5;241m=\u001b[39m (math\u001b[38;5;241m.\u001b[39msin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlander\u001b[38;5;241m.\u001b[39mangle), math\u001b[38;5;241m.\u001b[39mcos(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlander\u001b[38;5;241m.\u001b[39mangle))\n\u001b[1;32m    335\u001b[0m side \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m-\u001b[39mtip[\u001b[38;5;241m1\u001b[39m], tip[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m--> 336\u001b[0m dispersion \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnp_random\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muniform\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m SCALE \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m)]\n\u001b[1;32m    338\u001b[0m m_power \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontinuous \u001b[38;5;129;01mand\u001b[39;00m action[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontinuous \u001b[38;5;129;01mand\u001b[39;00m action \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    341\u001b[0m ):\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;66;03m# Main engine\u001b[39;00m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    # environment\n",
        "    env_name = 'LunarLander-v2'\n",
        "    n_state = 8\n",
        "    n_action = 4\n",
        "    train_dir = '.'\n",
        "    env_max_return = 300\n",
        "    env_step_limit = 1000\n",
        "    clip_limits = None\n",
        "    warmup_desired_return = 0\n",
        "    warmup_step_limit = 100\n",
        "\n",
        "    # agent\n",
        "    net_option = 'bilinear'\n",
        "    net_arch = (64, 128, 128)\n",
        "    action_type = 'discrete'\n",
        "    activation = 'relu'\n",
        "    return_scale = 0.015\n",
        "    horizon_scale = 0.03\n",
        "\n",
        "    # training & testing\n",
        "    replay = 'highest'\n",
        "    replay_size = 600\n",
        "    n_warm_up_episodes = 50\n",
        "    n_episodes_per_iter = 20\n",
        "    last_few = 100\n",
        "    learning_rate = 0.0008709635899560805\n",
        "    batch_size = 768\n",
        "    n_updates_per_iter = 150\n",
        "    max_training_steps = 10_000_000\n",
        "    eval_freq = 50_000\n",
        "    eval_goal = 'current'\n",
        "\n",
        "    train_alg = 'udrl'\n",
        "    train_seeds = (1_000_000, 10_000_000)\n",
        "    eval_seeds = (1, 500_000)\n",
        "    n_eval_episodes = 100\n",
        "    actions = 'on_policy'\n",
        "    save_model = True\n",
        "    verbose = False\n",
        "\n",
        "    # It seems that using CPU is faster\n",
        "    use_gpu = False\n",
        "    seed = 9\n",
        "\n",
        "\n",
        "ud = UpsideDownAgent(Config)\n",
        "ud.warm_up()\n",
        "print(f\"Warm-up complete. Starting training.\")\n",
        "eval_means, eval_medians = [], []\n",
        "\n",
        "while ud.total_steps < Config.max_training_steps:\n",
        "    results = ud.train_step()\n",
        "    for r in results:\n",
        "        if r[0] == 'eval.on_policy.mean':\n",
        "            eval_means.append(r[1])\n",
        "        if r[0] == 'eval.on_policy.median':\n",
        "            eval_medians.append(r[1])\n",
        "    ud.msg(f'Iteration {ud.iters} complete\\n')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "mv4NQbMZSgeT",
        "VOW8xhDXoiGH",
        "sCCOEthAqwrP",
        "nc1B-S01q-RI",
        "MynJcnRkrG2l"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "rl",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
